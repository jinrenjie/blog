---
title: 本地部署 LLM 和 Text Embedding 模型
date: 2024-09-29T12:00:02+08:00
tags: [LLM, Ollama, AI, Dify, RAG, Embedding]
draft: false
cover: covers/20240929-running-models-with-ollama.jpg
images:
  - covers/20240929-running-models-with-ollama.jpg
author: George
keywords: [LLM, Ollama, AI, Dify, RAG, Embedding]
description: 本文主使用 Dify 作为 RAG 的服务，后端模型 API 使用 Ollama 本地部署，零成本构建你自己的知识库……
readingTime: true
authorTwitter: GeorgeBornAgain
showFullContent: false
---

# 简介

如今 AI 领域的技术革新进行的如火如茶，诞生了很多新兴的项目，例如本文用到的 [Dify](https://dify.ai) 以及用于在本地部署大模型服务的 [Ollama](https://ollama.com)。

这两个项目可以让你零成本的在本地实现 RAG，以及基于知识库上下文的聊天问答。

# 系统要求

目前 Ollama 的 Windows 版尚处于 Preview 状态中，建议在 Apple M 系列处理器的电脑上运行，或者有 GPU 的 Linux Server。

最新的 llama3.2 对内存要求较上一代少了些，同时模型质量保持与 llama3.1 相当。据说是 2G+ 就可以了，甚至可以运行在一些移动设备上！

# 获取模型

访问 [Ollama](https://ollama.com/) 官网下载对应系统的 App，该 App 中包含 CLI 工具，然后再访问 [Models](https://ollama.com/library)，查看需要使用的模型，我这里使用的是最新的 `llama3.2`，以及 `mxbai-embed-large`。

模型的用途：

* llama3.2: 主要用于 Chat 文本生成
* mxbai-embed-large: 对文档进行 Embedding 操作

安装完成后查看 Ollama 的对应版本，有些模型对版本有特殊要求！

```shell
ollama -v
ollama version is 0.3.12
```

```shell
ollama pull llama3.2
ollama pull mxbai-embed-large
```

# 运行 Ollama 服务

运行 Ollama 服务的方式有两种，一种是直接打开 Ollama 的 App，另一种则是通过 CLI 来启动，当 App 运行后，CLI 就无法在运行了，因为这将导致服务绑定的端口冲突。

{{< prismjs lang=bash command-line=true prompt=$ output="2-29" >}}
ollama serve --help
Start ollama

Usage:
  ollama serve [flags]

Aliases:
  serve, start

Flags:
  -h, --help   help for serve

Environment Variables:
      OLLAMA_DEBUG               Show additional debug information (e.g. OLLAMA_DEBUG=1)
      OLLAMA_HOST                IP Address for the ollama server (default 127.0.0.1:11434)
      OLLAMA_KEEP_ALIVE          The duration that models stay loaded in memory (default "5m")
      OLLAMA_MAX_LOADED_MODELS   Maximum number of loaded models per GPU
      OLLAMA_MAX_QUEUE           Maximum number of queued requests
      OLLAMA_MODELS              The path to the models directory
      OLLAMA_NUM_PARALLEL        Maximum number of parallel requests
      OLLAMA_NOPRUNE             Do not prune model blobs on startup
      OLLAMA_ORIGINS             A comma separated list of allowed origins
      OLLAMA_SCHED_SPREAD        Always schedule model across all GPUs
      OLLAMA_TMPDIR              Location for temporary files
      OLLAMA_FLASH_ATTENTION     Enabled flash attention
      OLLAMA_LLM_LIBRARY         Set LLM library to bypass autodetection
      OLLAMA_GPU_OVERHEAD        Reserve a portion of VRAM per GPU (bytes)
      OLLAMA_LOAD_TIMEOUT        How long to allow model loads to stall before giving up (default "5m")
{{< /prismjs >}}

可以看到 Ollama 的服务支持通过 ENV 的方式去配置启动参数，这里建议设置如下环境变量：

* OLLAMA_HOST="0.0.0.0"
* OLLAMA_ORIGINS="*"
* OLLAMA_KEEP_ALIVE="24h"

{{< prismjs lang=bash command-line=true prompt=$ output="2-29" >}}
OLLAMA_HOST="0.0.0.0" OLLAMA_ORIGINS="*" OLLAMA_KEEP_ALIVE="24h" ollama serve
2024/09/29 12:37:38 routes.go:1153: INFO server config env="map[HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11434 OLLAMA_KEEP_ALIVE:24h0m0s OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/Users/George/.ollama/models OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[* http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://*] OLLAMA_SCHED_SPREAD:false OLLAMA_TMPDIR: http_proxy: https_proxy: no_proxy:]"
time=2024-09-29T12:37:38.522+08:00 level=INFO source=images.go:753 msg="total blobs: 17"
time=2024-09-29T12:37:38.523+08:00 level=INFO source=images.go:760 msg="total unused blobs removed: 0"
time=2024-09-29T12:37:38.523+08:00 level=INFO source=routes.go:1200 msg="Listening on [::]:11434 (version 0.3.12)"
time=2024-09-29T12:37:38.524+08:00 level=INFO source=common.go:135 msg="extracting embedded files" dir=/var/folders/p6/g0wwf37d165dfg5hvqtr9l180000gn/T/ollama966569504/runners
time=2024-09-29T12:37:38.547+08:00 level=INFO source=common.go:49 msg="Dynamic LLM libraries" runners=[metal]
time=2024-09-29T12:37:38.577+08:00 level=INFO source=types.go:107 msg="inference compute" id=0 library=metal variant="" compute="" driver=0.0 name="" total="48.0 GiB" available="48.0 GiB"
{{< /prismjs >}}

服务启动后，可以使用如下命令查看 Ollama 中现有的模型列表：

{{< prismjs lang=bash command-line=true prompt=$ output="2-42" >}}
curl http://127.0.0.1:11434/api/tags | jq .
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100  1391  100  1391    0     0   614k      0 --:--:-- --:--:-- --:--:--  679k
{
  "models": [
    {
      "name": "llama3.2:latest",
      "model": "llama3.2:latest",
      "modified_at": "2024-09-28T21:00:44.510551673+08:00",
      "size": 2019393189,
      "digest": "a80c4f17acd55265feec403c7aef86be0c25983ab279d83f3bcd3abbcb5b8b72",
      "details": {
        "parent_model": "",
        "format": "gguf",
        "family": "llama",
        "families": [
          "llama"
        ],
        "parameter_size": "3.2B",
        "quantization_level": "Q4_K_M"
      }
    },
    {
      "name": "mxbai-embed-large:latest",
      "model": "mxbai-embed-large:latest",
      "modified_at": "2024-07-04T16:35:42.71585157+08:00",
      "size": 669615493,
      "digest": "468836162de7f81e041c43663fedbbba921dcea9b9fefea135685a39b2d83dd8",
      "details": {
        "parent_model": "",
        "format": "gguf",
        "family": "bert",
        "families": [
          "bert"
        ],
        "parameter_size": "334M",
        "quantization_level": "F16"
      }
    }
  ]
}
{{< /prismjs >}}

# 运行模型

{{< prismjs lang=bash command-line=true prompt=$ output="2" >}}
ollama run llama3.2
>>> Send a message (/? for help)
{{< /prismjs >}}

运行上述命令后就成功运行 LLM 了，如果需要退出交互式命令行，可以使用快捷键 `Ctrl + D` 或者输入 `/bye` 退出。

退出后使用如下命令查看正在运行的模型：

{{< prismjs lang=bash command-line=true prompt=$ output="2-3" >}}
ollama ps
NAME               ID              SIZE      PROCESSOR    UNTIL
llama3.2:latest    a80c4f17acd5    4.0 GB    100% GPU     24 hours from now
{{< /prismjs >}}

> 注意：Text Embedding 模型不需要使用 `run` 命令手动运行，当模型 Pull 下来以后，Ollama 服务启动后，可以直接访问 API 调用 Text Embedding 模型!

尝试调用 Ollama 的 Text Embedding API：

{{< prismjs lang=bash command-line=true prompt=$ output="2-24" >}}
curl http://127.0.0.1:11434/api/embed -d '{
  "model": "mxbai-embed-large",
  "input": "Why is the sky blue?"
}' | jq .
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100 12889    0 12820  100    69   214k   1183 --:--:-- --:--:-- --:--:--  217k
{
  "model": "mxbai-embed-large",
  "embeddings": [
    [
      -0.010455716,
      -0.0025081504,
      0.014609501,
      .....
      -0.004249276,
      -0.010569806,
      -0.00813636
    ]
  ],
  "total_duration": 57530917,
  "load_duration": 708625,
  "prompt_eval_count": 6
}
{{< /prismjs >}}

在通过 API 调用 Text Embedding 模型后，模型将被 Ollama 自动载入，此时在运行 `ps` 命令查看时将得到如下结果：

{{< prismjs lang=bash command-line=true prompt=$ output="2-4" >}}
ollama ps
NAME                        ID              SIZE      PROCESSOR    UNTIL
mxbai-embed-large:latest    468836162de7    1.2 GB    100% GPU     24 hours from now
llama3.2:latest             a80c4f17acd5    4.0 GB    100% GPU     24 hours from now
{{< /prismjs >}}

# 总结

到这里本地部署模型已经完成了，接下来就是在 Dify 中使用 Ollama 中运行的 LLM 和 Text Embedding 模型配置自己的 RAG 应用了。

除了上述模型，你可以自行尝试其他模型，AI 领域的东西迭代的太快了，也许几天后又出了新的模型，降低资源占用的同时提升了模型质量。

我们能做的就是持续学习，紧跟前沿动态！

I hope this is helpful, Happy hacking...